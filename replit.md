# Kids Reading Platform - EPUB Processing Pipeline

## Overview
This project develops an EPUB processing pipeline for a kids' reading platform. It automates the download and transformation of books from Project Gutenberg into child-friendly formats, including the generation of comprehension questions and vocabulary, and stores them in a PostgreSQL database. The platform features AI-driven content categorization with grade-level benchmarks, a robust draft system for incremental processing, and an admin UI for content management and manual chapter splitting. The business vision is to create an engaging reading experience for children, leveraging AI to tailor content and support learning, with significant market potential in educational technology.

## User Preferences
The user prefers:
- Manual control over chapter splitting instead of AI automation
- Visual feedback for word count validation
- Simple, intuitive UI for book processing
- One task at a time processing (FIFO with priority ordering)

## Development Notes
- **Manual Server Restart Required:** Flask reloader is disabled (`use_reloader=False`) to prevent duplicate worker threads. Developers must manually restart the server after code changes.
- **Single Worker Guarantee:** The QueueManagerV2 runs exactly ONE worker thread that processes tasks sequentially in priority + FIFO order. Never run via `flask run` as it may re-enable the reloader.

## System Architecture

### UI/UX Decisions
The platform features a responsive admin UI designed for clarity and efficiency. The application uses a two-page structure: index.html serves as the main landing page with book download functionality and queue monitoring, while draft.html provides the full editing interface at /draft/<draft_id>. The index page features a centered new book download section where users enter a Gutenberg ID, a real-time AI queue status widget showing queued/processing/ready/error task counts (auto-refreshes every 5 seconds), and a sidebar for draft selection. After downloading a book, users are automatically redirected to the draft editor page. The draft page is focused solely on editing, with no download or queue widgets, featuring tag management, description editing, and the chapter builder. The chapter editor and book text are presented in a side-by-side layout, allowing seamless manual copy-pasting of HTML content with automatic preservation of formatting and images (embedded as base64 data URIs). Text usage tracking visually highlights paragraphs already used in chapters with unique color coding. The configuration settings include interactive tag chips with status indicators and manual editing capabilities, along with a dedicated save button for tags and cover URL. A separate queue monitoring page (queue.html) features a two-column layout showing active (queued/processing) tasks and completed (ready) tasks. Reading level defaults to "intermediate" for all books. The system supports multi-session workflows, automatically resuming progress when a draft is reloaded.

### Technical Implementations
The core is a Flask web application, structured using an application factory pattern with blueprints for modularity (app/routes/). It employs a Python-based EPUB parser for both text and HTML content, including image extraction. The EPUB parser preserves ALL original Gutenberg HTML formatting by extracting raw body innerHTML and sanitizing it with Bleach 6.x (using CSSSanitizer with comprehensive whitelists). This approach preserves all structural tags (div, span, pre, blockquote), all formatting tags (br, em, strong), all class attributes (user defines CSS at app level), all style attributes (margin-left, text-indent, etc.), all images (converted to base64 data URIs), and all tables with complete structure, while removing only dangerous elements (script, iframe, onclick handlers, etc.) for security. The chapter editor intelligently merges original formatting with usage tracking highlights (with proper semicolon normalization to prevent invalid CSS), and the copy/paste system strips only the specific usage tracking styles (both hex and rgb color formats) while preserving all original formatting attributes. Question, vocabulary, and description generation are handled by an Ollama LLM and executed through a database-first queue system (QueueManagerV2) that uses PostgreSQL atomic row locking (SELECT FOR UPDATE SKIP LOCKED) for safe task coordination. The queue persists tasks in the `queue_tasks` table with columns for id, task_type, priority, status, book_id, chapter_id, payload (JSONB), attempts, locked_at, timeout_at, error_message, and timestamps. Tasks are executed by direct Ollama executor functions (src/queue_executors.py) that call the LLM with use_queue=False to prevent nested queueing. A 15-minute timeout watchdog thread monitors processing tasks and marks stuck tasks as errors. Status calculation is now dynamic (src/status_calculator.py) - tags/descriptions check if data exists, then query the queue; chapter questions require ALL expected questions (num_grades × 3) to be marked ready. AI-powered tag generation uses only book title and author for faster processing. Description generation is a two-step AI process: synopsis generation from initial book text followed by child-friendly description creation. The database schema uses ON DELETE CASCADE for automatic cleanup when books/chapters are deleted. Database-level unique constraints (migration 013) prevent duplicate queue tasks using partial indexes on (task_type, book_id, chapter_id, payload->>'grade_level') for question tasks and (task_type, book_id) for book-level tasks, ensuring race-free concurrent operations with INSERT...ON CONFLICT DO NOTHING. The contenteditable div for chapters syncs both plain text and HTML content. The LLM integration is model-agnostic, supporting various Ollama models with intelligent retry strategies and robust JSON extraction. GPU-optimized processing is controlled via the `PARALLEL_GENERATION` environment variable. Text usage tracking uses rapidfuzz hybrid fuzzy matching (60% Levenshtein + 40% token_sort_ratio with 78% threshold) to identify which book paragraphs have been used in chapters, enabling visual highlighting in the UI with CSS pseudo-elements (::before) to prevent background color copying. Chapter titles are required fields and do not auto-populate to prevent carryover between books.

### Feature Specifications
- **Draft System:** Supports incremental processing, auto-saving, async question generation with real-time updates, and uniqueness enforcement by Gutenberg ID.
- **Queue Management:** Real-time monitoring and control of the Ollama AI queue via a header widget and a dedicated full-page monitor, including task type, priority, and status, with flush functionality. Queue persistence ensures tasks survive server restarts by storing them in a PostgreSQL database table. Database-first design with automatic conflict deletion ensures no duplicate tasks. Watchdog thread provides automatic recovery for timed-out tasks.
- **Dynamic Status Calculation:** Real-time status computed on-demand from data existence and queue state (no persistent status columns). Tags and descriptions return 'ready' when data exists, otherwise query queue for 'queued'/'processing'/'error'. Chapter questions require ALL expected questions (num_grades × 3 questions per grade) to be marked 'ready'. Status flows: pending → queued → processing → ready/error.
- **Chapter Management:** Manual chapter splitting through the UI, with word count validation, fully editable HTML content, and required chapter titles (no auto-populated defaults).
- **AI-Powered Tagging:** Asynchronous generation of grade-level and genre tags from book title and author, with status tracking.
- **AI-Powered Description Generation:** Asynchronous, two-step generation of child-friendly book descriptions, with manual editing and regeneration options.
- **EPUB Image Extraction:** Automatic extraction and embedding of EPUB images as base64 data URIs in HTML content.
- **Text Usage Tracking:** Frontend-based fuzzy text matching to automatically highlight saved chapter text in the book display. When a chapter is saved, the system performs word-based sequential matching (60% threshold) to find the corresponding paragraphs in the book. Each saved chapter is highlighted with a unique color (5-color rotation). The system handles paragraph reassignment when text is reused across chapters, ensuring the most recently saved chapter controls the highlighting. Uses normalized text comparison (lowercase, whitespace normalization, punctuation removal) for accurate matching despite HTML formatting differences.
- **Multi-Grade Question Generation:** Generates grade-specific questions and vocabulary for each detected grade level.
- **Grade-Specific Vocabulary:** Generates 8 vocabulary words per grade, with definitions and examples tailored to that grade.
- **Automatic Question Regeneration:** Smart regeneration of questions triggered by tag changes or manual edits, processing only affected grades. The "Regenerate All Questions" button creates n_chapters × n_grades × 3 tasks (3 separate tasks per grade per chapter), where each task generates 1 question. Existing queued question tasks are deleted before new tasks are enqueued.
- **Chapter-Level Question Regeneration:** Independent regeneration of questions for individual chapters.
- **Tag Regeneration:** AI-powered regeneration of tags, triggering question regeneration, with duplicate request protection.
- **Vocabulary Tooltips:** Interactive vocabulary definitions embedded in HTML.
- **Book Cover URL:** Option to add a cover image URL.
- **Reading Benchmarks:** Modal display of reading benchmarks by grade level.

### System Design Choices
The architecture emphasizes separation of concerns (EPUB parser, question generator, database manager). Data persistence is handled by PostgreSQL with `ON DELETE CASCADE` for integrity. AI prompt engineering and robust parsing ensure consistent LLM output. A database-first queue system (QueueManagerV2) orchestrates all LLM API calls using PostgreSQL's SELECT FOR UPDATE SKIP LOCKED for atomic task locking, enabling safe concurrent processing and horizontal scalability. The singleton manager runs separate worker and watchdog threads—the worker claims and executes tasks via direct Ollama executors, while the watchdog monitors timeout_at to recover stuck tasks. Tasks are prioritized (1=tags, 2=descriptions, 3=questions) and stored in the queue_tasks table with JSONB payloads, surviving server restarts. Status is calculated dynamically on every API request by checking data existence first, then querying the queue, avoiding drift and ensuring real-time accuracy. This design prevents deadlocks (direct execution, no nested queueing), ensures task durability (database-backed), and provides automatic cleanup (ON DELETE CASCADE).

## External Dependencies
- **PostgreSQL:** Primary database for all project data.
- **Ollama:** Large Language Model (LLM) for content generation (questions, vocabulary, tags, descriptions).
- **Project Gutenberg:** Source for downloading EPUB books.
- **Flask:** Python web framework for the backend application and API.