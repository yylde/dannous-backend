# Architecture Documentation

## Database Schema Interpretation

This document explains how the existing database schema (from Alembic migrations) was analyzed and mapped to the EPUB processing pipeline.

## Schema Analysis Process

### 1. Source: Alembic Migration File

Located at: `alembic/versions/29aa4061a4be_initial_schema.py`

This migration file contains the complete database schema with all table definitions, constraints, and relationships.

### 2. Relevant Tables for Book Processing

The pipeline interacts with three primary tables:

#### **books** Table

**Purpose:** Store book metadata and overview information

**Schema Mapping:**
```sql
CREATE TABLE books (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    title VARCHAR(500) NOT NULL,
    author VARCHAR(300) NOT NULL,
    description TEXT,
    age_range VARCHAR(20) NOT NULL,
    reading_level VARCHAR(20) NOT NULL,
    genre VARCHAR(100),
    total_chapters INTEGER NOT NULL,
    estimated_reading_time_minutes INTEGER,
    cover_image_url VARCHAR(500),
    isbn VARCHAR(20),
    publication_year INTEGER,
    created_at TIMESTAMP DEFAULT NOW(),
    is_active BOOLEAN DEFAULT true,
    content_rating VARCHAR(20),
    tags JSONB DEFAULT '[]'
);
```

**Pipeline Mapping:**
- `title` ← EPUB metadata (DC:title)
- `author` ← EPUB metadata (DC:creator)
- `description` ← First paragraph(s) of cleaned text
- `age_range` ← User input (CLI parameter)
- `reading_level` ← User input (CLI parameter)
- `genre` ← User input or EPUB metadata
- `total_chapters` ← Calculated from chapter splitter output
- `estimated_reading_time_minutes` ← Sum of all chapter reading times
- `isbn` ← EPUB metadata (DC:identifier)
- `publication_year` ← EPUB metadata (DC:date) - extracted year
- `tags` ← Empty array (could be populated from genre/metadata)
- `content_rating` ← NULL (could be inferred from age_range)
- `cover_image_url` ← NULL (not extracted from EPUB)

#### **chapters** Table

**Purpose:** Store individual chapter/section content

**Schema Mapping:**
```sql
CREATE TABLE chapters (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    book_id UUID NOT NULL REFERENCES books(id) ON DELETE CASCADE,
    chapter_number INTEGER NOT NULL,
    title VARCHAR(300) NOT NULL,
    content TEXT NOT NULL,
    word_count INTEGER NOT NULL,
    estimated_reading_time_minutes INTEGER NOT NULL,
    vocabulary_words JSONB DEFAULT '[]',
    created_at TIMESTAMP DEFAULT NOW(),
    UNIQUE(book_id, chapter_number)
);
```

**Pipeline Mapping:**
- `book_id` ← Generated book UUID (foreign key relationship)
- `chapter_number` ← Sequential numbering from chapter splitter
- `title` ← Detected from text patterns or auto-generated ("Section N")
- `content` ← Cleaned chapter text from splitter
- `word_count` ← Calculated from content.split()
- `estimated_reading_time_minutes` ← word_count / 200 WPM
- `vocabulary_words` ← Empty array (future: could extract difficult words)

**Key Constraint:** `UNIQUE(book_id, chapter_number)` prevents duplicate chapters

#### **questions** Table

**Purpose:** Store comprehension questions for each chapter

**Schema Mapping:**
```sql
CREATE TABLE questions (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    book_id UUID NOT NULL REFERENCES books(id) ON DELETE CASCADE,
    chapter_id UUID NOT NULL REFERENCES chapters(id) ON DELETE CASCADE,
    question_text TEXT NOT NULL,
    question_type VARCHAR(50) NOT NULL DEFAULT 'comprehension',
    difficulty_level VARCHAR(20) DEFAULT 'medium',
    expected_keywords JSONB DEFAULT '[]',
    min_word_count INTEGER DEFAULT 20,
    max_word_count INTEGER DEFAULT 200,
    order_index INTEGER NOT NULL,
    is_active BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT NOW()
);
```

**Pipeline Mapping:**
- `book_id` ← Same book UUID as chapters
- `chapter_id` ← Chapter UUID for this specific chapter
- `question_text` ← Generated by Ollama LLM
- `question_type` ← Always 'comprehension' (could vary in future)
- `difficulty_level` ← Extracted from LLM response ('easy', 'medium', 'hard')
- `expected_keywords` ← Extracted from LLM response as JSON array
- `min_word_count` ← Default 20 (from config)
- `max_word_count` ← Default 200 (from config)
- `order_index` ← Sequential: 1, 2, 3 per chapter

## Data Flow Architecture

```
┌─────────────────┐
│  EPUB File      │
│  (.epub)        │
└────────┬────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│  1. EPUB Parser (epub_parser.py)        │
│  - Extracts metadata (title, author)    │
│  - Extracts HTML content                │
│  - Combines into raw text               │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│  2. Text Cleaner (text_cleaner.py)      │
│  - Removes PG header/footer             │
│  - Removes license boilerplate          │
│  - Normalizes whitespace                │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│  3. Chapter Splitter (chapter_splitter) │
│  - Detects chapter boundaries           │
│  - Splits by reading level limits       │
│  - Calculates word counts               │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│  4. Question Generator (Ollama)         │
│  - For each chapter:                    │
│    - Build prompt with context          │
│    - Call Ollama API                    │
│    - Parse JSON response                │
│    - Extract questions + keywords       │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│  5. Data Model Builder (models.py)      │
│  - Create Book object                   │
│  - Create Chapter objects               │
│  - Create Question objects              │
│  - Link via UUIDs (foreign keys)        │
└────────┬────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│  6. Database Insertion (database.py)    │
│  - Begin transaction                    │
│  - Insert book                          │
│  - Insert chapters (in order)           │
│  - Insert questions (per chapter)       │
│  - Commit transaction                   │
└─────────────────────────────────────────┘
         │
         ▼
┌─────────────────────────────────────────┐
│  PostgreSQL Database                    │
│  - books table                          │
│  - chapters table (ON DELETE CASCADE)   │
│  - questions table (ON DELETE CASCADE)  │
└─────────────────────────────────────────┘
```

## Key Design Decisions

### 1. UUID Generation

**Decision:** Generate UUIDs in Python (not database)

**Rationale:**
- Need UUIDs before insertion to establish relationships
- Allows for foreign key setup between Book → Chapter → Question
- Python's `uuid4()` provides sufficient uniqueness

**Implementation:**
```python
from uuid import uuid4

book = Book(id=uuid4(), ...)
chapter = Chapter(id=uuid4(), book_id=book.id, ...)
question = Question(id=uuid4(), chapter_id=chapter.id, ...)
```

### 2. Transaction Management

**Decision:** Use single transaction for entire book insertion

**Rationale:**
- Ensures atomicity (all or nothing)
- Prevents partial book data in database
- Leverages PostgreSQL's CASCADE DELETE

**Implementation:**
```python
with conn:  # Auto-commit on success, rollback on error
    insert_book(book)
    for chapter in chapters:
        insert_chapter(chapter)
    for question in questions:
        insert_question(question)
```

### 3. Duplicate Detection

**Decision:** Check for duplicates by (title, author) before insertion

**Rationale:**
- Prevents accidental re-processing
- Unique constraint not in schema, so manual check needed
- Early detection saves processing time

**Implementation:**
```python
existing = db.check_duplicate(book.title, book.author)
if existing:
    raise ValueError(f"Book already exists: {existing}")
```

### 4. Reading Level-Based Chapter Splitting

**Decision:** Different max word counts per reading level

**Rationale:**
- Younger readers need shorter chunks
- Matches educational best practices
- Configurable via environment variables

**Configuration:**
```env
MAX_CHAPTER_WORDS_BEGINNER=800      # 6-8 years
MAX_CHAPTER_WORDS_INTERMEDIATE=1500 # 8-12 years
MAX_CHAPTER_WORDS_ADVANCED=2500     # 12+ years
```

### 5. Question Keyword Extraction

**Decision:** Store expected keywords as JSONB array

**Rationale:**
- Supports answer validation in main platform
- Flexible structure (varying keyword counts)
- Efficient querying with PostgreSQL JSONB operators

**Schema Design:**
```sql
expected_keywords JSONB DEFAULT '[]'
```

**Example Data:**
```json
["motivation", "decision", "consequence", "character", "emotion"]
```

## Cascade Delete Behavior

The schema uses `ON DELETE CASCADE` for referential integrity:

```
DELETE books WHERE id='xxx'
  ↓ CASCADE
  ├─ DELETE chapters WHERE book_id='xxx'
  │    ↓ CASCADE
  │    └─ DELETE questions WHERE chapter_id IN (...)
  └─ DELETE questions WHERE book_id='xxx'
```

**Implication for Pipeline:**
- No need to manually delete chapters/questions
- Simplifies cleanup of failed/duplicate insertions
- Must be careful with deletions (permanent)

## Error Handling Strategy

### Database Errors

```python
try:
    db.insert_processed_book(book)
except ValueError as e:
    # Expected error (duplicate book)
    console.print(f"Book already exists: {e}")
except psycopg2.IntegrityError as e:
    # Constraint violation
    conn.rollback()
    raise
except Exception as e:
    # Unexpected error
    conn.rollback()
    logger.exception("Database insertion failed")
    raise
```

### Validation Errors

- Title/author max length (VARCHAR constraints)
- Chapter number uniqueness per book
- Required fields (NOT NULL)
- Data type mismatches

## Performance Considerations

### 1. Batch Inserts

**Current:** Individual INSERTs per chapter/question

**Optimization Opportunity:**
```python
# Could use execute_values for bulk insert
from psycopg2.extras import execute_values

execute_values(
    cursor,
    "INSERT INTO chapters (id, book_id, ...) VALUES %s",
    [(c.id, c.book_id, ...) for c in chapters]
)
```

### 2. Connection Pooling

**Current:** Single connection per operation

**For Production:**
```python
from psycopg2 import pool

connection_pool = pool.ThreadedConnectionPool(
    minconn=1,
    maxconn=20,
    dsn=database_url
)
```

### 3. Index Usage

**Existing Indexes:**
- Primary keys (id) - automatic B-tree
- Unique constraint (book_id, chapter_number) - automatic

**Potential Additions:**
```sql
-- For book lookups
CREATE INDEX idx_books_title_author ON books(title, author);

-- For chapter queries
CREATE INDEX idx_chapters_book_id ON chapters(book_id);

-- For question queries
CREATE INDEX idx_questions_chapter_id ON questions(chapter_id);
```

## Testing Strategy

### Unit Tests

- `test_epub_parser.py` - EPUB extraction
- `test_text_cleaner.py` - Boilerplate removal
- `test_chapter_splitter.py` - Chapter detection/splitting
- `test_question_generator.py` - Ollama integration

### Integration Tests

- `test_pipeline.py` - End-to-end processing
- `test_database.py` - Database operations

### Test Database

Recommendation: Use separate test database

```bash
createdb reading_platform_test
export DATABASE_URL_TEST="postgresql://localhost/reading_platform_test"
pytest tests/
```

## Future Enhancements

### Potential Schema Extensions

1. **Vocabulary Extraction:**
   ```python
   vocabulary_words = extract_difficult_words(chapter_content, reading_level)
   chapter.vocabulary_words = vocabulary_words  # Populate JSONB field
   ```

2. **Content Rating:**
   ```python
   content_rating = infer_rating(age_range, content)
   book.content_rating = content_rating  # Use existing field
   ```

3. **Tags/Topics:**
   ```python
   tags = extract_topics(book_content)  # Using NLP
   book.tags = tags  # Use existing JSONB field
   ```

4. **Cover Images:**
   ```python
   cover_url = extract_epub_cover(epub_file)
   book.cover_image_url = cover_url  # Use existing field
   ```

## Conclusion

The pipeline successfully maps EPUB content to the existing database schema by:

1. ✅ Analyzing Alembic migration files for schema structure
2. ✅ Mapping EPUB metadata to book fields
3. ✅ Processing text into age-appropriate chapters
4. ✅ Generating questions that match question schema
5. ✅ Maintaining referential integrity with UUIDs
6. ✅ Respecting constraints (UNIQUE, NOT NULL, CASCADE)
7. ✅ Using transactions for data consistency

The architecture is modular, testable, and respects the existing database design while adding valuable automated content processing capabilities.